{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from rdflib import Graph, URIRef, Literal, Namespace, XSD\n",
    "from rdflib.namespace import RDF, RDFS, DCTERMS, SKOS\n",
    "import requests\n",
    "import spotlight\n",
    "from multiprocessing.dummy import Pool as ThreadPool  # Usamos hilos en lugar de procesos\n",
    "\n",
    "# Namespaces\n",
    "schema = Namespace(\"https://schema.org/\")\n",
    "dbpedia = Namespace(\"http://dbpedia.org/resource/\")\n",
    "dbo = Namespace(\"http://dbpedia.org/ontology/\")\n",
    "myv = Namespace(\"http://example.org/myv/\")\n",
    "\n",
    "# Configuración del endpoint de DBpedia Spotlight\n",
    "DBPEDIA_ENDPOINT = 'https://api.dbpedia-spotlight.org/en/'\n",
    "\n",
    "# Caché de respuestas para DBpedia\n",
    "dbpedia_cache = {}\n",
    "\n",
    "def get_annotations(text, confidence=0.4, support=20):\n",
    "    \"\"\" Obtiene anotaciones de un texto usando DBpedia Spotlight. \"\"\"\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        print(f\"Anotando texto: {text[:100]}...\")\n",
    "        annotations = spotlight.annotate(DBPEDIA_ENDPOINT + 'annotate', text, confidence=confidence, support=support)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Se encontraron {len(annotations)} anotaciones en {elapsed_time:.2f} segundos.\")\n",
    "        return [\n",
    "            {\n",
    "                'uri': a['URI'],\n",
    "                'surfaceForm': a['surfaceForm'],\n",
    "                'similarityScore': a['similarityScore']\n",
    "            }\n",
    "            for a in annotations\n",
    "        ]\n",
    "    except Exception as e:\n",
    "        print(f\"Error en DBpedia Spotlight: {e}\")\n",
    "        return []\n",
    "\n",
    "def get_dbpedia_info(uri):\n",
    "    \"\"\" Consulta DBpedia para obtener información adicional de una URI. \"\"\"\n",
    "    if uri in dbpedia_cache:\n",
    "        return dbpedia_cache[uri]  # Usamos caché para evitar consultas repetidas\n",
    "\n",
    "    print(f\"Consultando DBpedia para la URI: {uri}\")\n",
    "    query = f\"\"\"\n",
    "    SELECT ?prop ?value WHERE {{\n",
    "        VALUES ?uri {{ <{uri}> }}\n",
    "        VALUES ?prop {{ rdfs:label rdfs:comment dcterms:subject }}\n",
    "        ?uri ?prop ?value .\n",
    "        FILTER(LANG(?value) = \"es\" || LANG(?value) = \"en\")\n",
    "    }}\n",
    "    \"\"\"\n",
    "    sparql_endpoint = \"http://dbpedia.org/sparql\"\n",
    "    params = {\"query\": query, \"format\": \"json\"}\n",
    "\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        response = requests.get(sparql_endpoint, params=params, timeout=5)  # Tiempo máximo de espera\n",
    "        elapsed_time = time.time() - start_time\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            results = [(result[\"prop\"][\"value\"], result[\"value\"][\"value\"]) for result in data[\"results\"][\"bindings\"]]\n",
    "            dbpedia_cache[uri] = results  # Guardamos en caché\n",
    "            print(f\"Información encontrada para {uri} en {elapsed_time:.2f} segundos.\")\n",
    "            return results\n",
    "        else:\n",
    "            print(f\"Error SPARQL ({response.status_code}): {response.text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error consultando DBpedia: {e}\")\n",
    "    \n",
    "    return []\n",
    "\n",
    "def process_article(article, description):\n",
    "    \"\"\" Procesa un artículo y su descripción para enriquecer el grafo. \"\"\"\n",
    "    start_time = time.time()\n",
    "    enriched_graph = Graph()\n",
    "    print(f\"\\nProcesando artículo: {article}\")\n",
    "    print(f\"Descripción: {description[:3000]}...\")\n",
    "\n",
    "    # Anotaciones con DBpedia Spotlight\n",
    "    annotations = get_annotations(str(description))\n",
    "    for annotation in annotations[:5]:  # Reducimos a 5 anotaciones por artículo\n",
    "        dbpedia_uri = URIRef(annotation['uri'])\n",
    "        surface_form = annotation['surfaceForm']\n",
    "        similarity_score = annotation['similarityScore']\n",
    "\n",
    "        enriched_graph.add((article, schema.mentions, dbpedia_uri))\n",
    "        enriched_graph.add((dbpedia_uri, RDFS.label, Literal(surface_form)))\n",
    "        enriched_graph.add((dbpedia_uri, myv.similarityScore, Literal(similarity_score, datatype=XSD.float)))\n",
    "\n",
    "        # Obtener información adicional de DBpedia\n",
    "        info = get_dbpedia_info(dbpedia_uri)\n",
    "        for prop, value in info:\n",
    "            enriched_graph.add((dbpedia_uri, URIRef(prop), Literal(value)))\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Procesamiento de {article} completado en {elapsed_time:.2f} segundos.\")\n",
    "    return enriched_graph\n",
    "\n",
    "# Cargar el grafo RDF original\n",
    "input_file = \"articles_data_cleaned_final_1.ttl\"\n",
    "print(f\"\\nCargando grafo RDF desde {input_file}...\")\n",
    "g = Graph()\n",
    "try:\n",
    "    g.parse(input_file, format=\"turtle\")\n",
    "    print(f\"Grafo cargado correctamente. Número de triples: {len(g)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al cargar el grafo RDF: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Preparar el grafo enriquecido\n",
    "enriched_graph = Graph()\n",
    "enriched_graph.bind(\"dbo\", dbo)\n",
    "enriched_graph.bind(\"dbpedia\", dbpedia)\n",
    "enriched_graph.bind(\"myv\", myv)\n",
    "enriched_graph.bind(\"skos\", SKOS)\n",
    "enriched_graph.bind(\"schema\", schema)\n",
    "\n",
    "# Extraer artículos y sus descripciones (limitado a 10 artículos para pruebas)\n",
    "articles = list(g.subjects(RDF.type, schema.Article))[:3000]  \n",
    "print(f\"\\nSe procesarán {len(articles)} artículos.\")\n",
    "\n",
    "# Procesar artículos en paralelo usando hilos\n",
    "print(\"\\nIniciando procesamiento de artículos...\")\n",
    "try:\n",
    "    with ThreadPool(processes=6) as pool:  # 4 hilos en lugar de procesos\n",
    "        results = pool.starmap(process_article, [(article, g.value(article, schema.description)) for article in articles if g.value(article, schema.description)])\n",
    "    print(\"\\nProcesamiento de artículos completado.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error en procesamiento paralelo: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Combinar los resultados en el grafo enriquecido\n",
    "print(\"\\nCombinando resultados en el grafo enriquecido...\")\n",
    "for result in results:\n",
    "    enriched_graph += result\n",
    "print(f\"Grafo enriquecido listo. Número de triples: {len(enriched_graph)}\")\n",
    "\n",
    "# Guardar el grafo enriquecido\n",
    "output_file = \"enriched_articles_data_with_annotations_2.ttl\"\n",
    "print(f\"\\nGuardando grafo enriquecido en {output_file}...\")\n",
    "try:\n",
    "    enriched_graph.serialize(output_file, format=\"turtle\")\n",
    "    print(f\"Grafo enriquecido guardado correctamente.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al guardar el grafo: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CORRECCIÓN DE URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo corregido guardado como enriched_annotations.ttl\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Ruta del archivo original y del archivo corregido\n",
    "input_file = \"pre_enriched_annotations.ttl\"\n",
    "output_file = \"enriched_annotations.ttl\"\n",
    "\n",
    "# Expresión regular para encontrar y reemplazar en URIs específicas\n",
    "pattern = re.compile(r\"<http://example.org/article/(.*?)>\")\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as infile, open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for line in infile:\n",
    "        modified_line = pattern.sub(r\"<http://example.org/sa/\\1>\", line)\n",
    "        outfile.write(modified_line)\n",
    "\n",
    "print(\"Archivo corregido guardado como\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook enrequecimiento_annotation.ipynb to html\n",
      "[NbConvertApp] Writing 299528 bytes to enrequecimiento_annotation.html\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to html enrequecimiento_annotation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
